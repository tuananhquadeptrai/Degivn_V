# %% [markdown]
# # Devign Model Training Pipeline - BiGRU
# 
# Train vulnerability detection models on preprocessed Devign dataset.
# 
# **Environment**: Kaggle with 2x NVIDIA T4 GPU (32GB total VRAM), 13GB RAM
# 
# **Model**: BiGRU with Additive Attention
# 
# **Features**:
# - Multi-GPU training with DataParallel
# - Mixed precision training (AMP)
# - Gradient accumulation
# - OneCycleLR / ReduceLROnPlateau scheduling
# - Early stopping & checkpointing
# - Label smoothing option
# - Comprehensive metrics (Accuracy, Precision, Recall, F1, AUC-ROC)

# %% [markdown]
# ## 1. Setup & Configuration

# %%
import os
import sys
import time
import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.cuda.amp import autocast, GradScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, confusion_matrix, classification_report
)
from tqdm.auto import tqdm

# Environment setup
if os.path.exists('/kaggle'):
    WORKING_DIR = '/kaggle/working'
    DATA_DIR = '/kaggle/input/devign-final/processed'
    sys.path.insert(0, '/kaggle/working/devign_pipeline')
else:
    WORKING_DIR = '/media/hdi/Hdii/Work/C Vul Devign'
    DATA_DIR = '/media/hdi/Hdii/Work/C Vul Devign/Dataset/devign slice'
    sys.path.insert(0, '/media/hdi/Hdii/Work/C Vul Devign/devign_pipeline')

MODEL_DIR = os.path.join(WORKING_DIR, 'models')
LOG_DIR = os.path.join(WORKING_DIR, 'logs')

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# Device setup
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
N_GPUS = torch.cuda.device_count()

print(f"Device: {DEVICE}")
print(f"GPU count: {N_GPUS}")
if torch.cuda.is_available():
    for i in range(N_GPUS):
        gpu = torch.cuda.get_device_properties(i)
        print(f"  GPU {i}: {gpu.name} ({gpu.total_memory / 1024**3:.1f} GB)")

# %% [markdown]
# ## 2. Training Configuration

# %%
def load_data_config(data_dir: str) -> Dict:
    """Load config from preprocessed data"""
    config_path = os.path.join(data_dir, 'config.json')
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            return json.load(f)
    return {}

data_config = load_data_config(DATA_DIR)

class TrainConfig:
    """Training hyperparameters - optimized for Devign dataset"""
    # Model (optimized for vocab_size=266, 21K samples)
    vocab_size: int = data_config.get('vocab_size', 266)
    embed_dim: int = 128          # Reduced from 256 for small vocab
    hidden_dim: int = 256         # BiGRU output = 512
    num_layers: int = 2
    rnn_dropout: float = 0.3      # Dropout between GRU layers
    classifier_dropout: float = 0.5  # Stronger dropout in classifier
    bidirectional: bool = True
    
    # Training
    batch_size: int = 64
    accumulation_steps: int = 2   # Effective batch size = 128
    learning_rate: float = 1e-3
    max_lr: float = 2e-3          # For OneCycleLR
    weight_decay: float = 5e-3    # Increased from 1e-5
    max_epochs: int = 50
    grad_clip: float = 1.0
    
    # Regularization
    label_smoothing: float = 0.05  # Helps with noisy labels
    
    # Early stopping
    patience: int = 7
    min_delta: float = 1e-4
    
    # Data
    max_seq_length: int = 512
    num_workers: int = 4
    
    # Checkpointing
    save_every: int = 5
    
    # Mixed precision
    use_amp: bool = True
    
    # Scheduler: 'onecycle' or 'plateau'
    scheduler_type: str = 'onecycle'
    
    def to_dict(self) -> Dict:
        return {k: v for k, v in self.__class__.__dict__.items() 
                if not k.startswith('_') and not callable(v)}

config = TrainConfig()
print("Training config:")
for k, v in config.to_dict().items():
    print(f"  {k}: {v}")

# %% [markdown]
# ## 3. Dataset & DataLoader

# %%
class DevignDataset(Dataset):
    """Load preprocessed single .npz file"""
    
    def __init__(self, npz_path: str, max_seq_length: int = 512):
        self.max_seq_length = max_seq_length
        
        print(f"Loading {npz_path}...")
        data = np.load(npz_path)
        self.input_ids = data['input_ids']
        self.labels = data['labels']
        
        # Handle attention_mask if present, otherwise generate
        if 'attention_mask' in data:
            self.attention_mask = data['attention_mask']
        else:
            self.attention_mask = None
        
        print(f"  Loaded {len(self.labels)} samples")
    
    def __len__(self) -> int:
        return len(self.labels)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        input_ids = self.input_ids[idx][:self.max_seq_length]
        
        # Pad if needed
        padding_length = self.max_seq_length - len(input_ids)
        if padding_length > 0:
            input_ids = np.pad(input_ids, (0, padding_length), constant_values=0)
        
        # Attention mask (1 for real tokens, 0 for padding)
        if self.attention_mask is not None:
            attention_mask = self.attention_mask[idx][:self.max_seq_length]
            if len(attention_mask) < self.max_seq_length:
                attention_mask = np.pad(attention_mask, (0, self.max_seq_length - len(attention_mask)), constant_values=0)
        else:
            attention_mask = (input_ids != 0).astype(np.float32)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.float),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }


def create_dataloaders(
    data_dir: str, 
    batch_size: int, 
    max_seq_length: int,
    num_workers: int = 4
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """Create train/val/test dataloaders - supports single npz files"""
    
    # Check for single npz files first
    train_path = Path(data_dir) / 'train.npz'
    val_path = Path(data_dir) / 'val.npz'
    test_path = Path(data_dir) / 'test.npz'
    
    if train_path.exists():
        print("Found single npz files")
        train_dataset = DevignDataset(str(train_path), max_seq_length)
        val_dataset = DevignDataset(str(val_path), max_seq_length)
        test_dataset = DevignDataset(str(test_path), max_seq_length)
    else:
        # Fallback to chunked files
        train_paths = sorted(Path(data_dir).glob('train/*.npz'))
        val_paths = sorted(Path(data_dir).glob('val/*.npz'))
        test_paths = sorted(Path(data_dir).glob('test/*.npz'))
        
        print(f"Found: {len(train_paths)} train, {len(val_paths)} val, {len(test_paths)} test chunks")
        raise ValueError("Chunked loading not implemented - use single npz files")
    
    print(f"Samples: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test")
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, 
        num_workers=num_workers, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )
    
    return train_loader, val_loader, test_loader

# %%
train_loader, val_loader, test_loader = create_dataloaders(
    DATA_DIR, 
    config.batch_size, 
    config.max_seq_length,
    config.num_workers
)

# Test batch
batch = next(iter(train_loader))
print(f"\nBatch shapes:")
print(f"  input_ids: {batch['input_ids'].shape}")
print(f"  attention_mask: {batch['attention_mask'].shape}")
print(f"  labels: {batch['labels'].shape}")

# Class distribution
train_labels = train_loader.dataset.labels
print(f"\nClass distribution: 0={np.sum(train_labels==0)}, 1={np.sum(train_labels==1)}")

# %% [markdown]
# ## 4. BiGRU Model with Attention

# %%
class BiGRUVulnDetector(nn.Module):
    """Bidirectional GRU with Additive Attention for vulnerability detection
    
    Architecture:
    - Embedding layer with dropout
    - 2-layer BiGRU
    - Additive attention pooling
    - 2-layer MLP classifier with dropout
    """
    
    def __init__(
        self,
        vocab_size: int,
        embed_dim: int = 128,
        hidden_dim: int = 256,
        num_layers: int = 2,
        rnn_dropout: float = 0.3,
        classifier_dropout: float = 0.5,
        bidirectional: bool = True,
        num_classes: int = 2,
        padding_idx: int = 0
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional
        
        # Embedding with dropout
        self.embedding = nn.Embedding(
            vocab_size, embed_dim, padding_idx=padding_idx
        )
        self.embed_dropout = nn.Dropout(0.1)
        
        # BiGRU (faster and fewer params than LSTM)
        self.gru = nn.GRU(
            embed_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=rnn_dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        gru_output_dim = hidden_dim * 2 if bidirectional else hidden_dim
        
        # Additive attention
        self.attention = nn.Sequential(
            nn.Linear(gru_output_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        # MLP classifier with strong dropout
        self.classifier = nn.Sequential(
            nn.Dropout(classifier_dropout),
            nn.Linear(gru_output_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(classifier_dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights with Xavier/Kaiming"""
        for name, param in self.named_parameters():
            if 'embedding' in name:
                nn.init.normal_(param, mean=0, std=0.02)
            elif 'weight_ih' in name:  # GRU input weights
                nn.init.xavier_uniform_(param)
            elif 'weight_hh' in name:  # GRU hidden weights
                nn.init.orthogonal_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
            elif 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
    
    def forward(
        self, 
        input_ids: torch.Tensor, 
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        # Embed with dropout
        embeds = self.embedding(input_ids)  # (B, L, E)
        embeds = self.embed_dropout(embeds)
        
        # BiGRU
        gru_out, _ = self.gru(embeds)  # (B, L, H*2)
        
        # Attention pooling
        attn_weights = self.attention(gru_out)  # (B, L, 1)
        attn_weights = attn_weights.masked_fill(
            attention_mask.unsqueeze(-1) == 0, float('-inf')
        )
        attn_weights = torch.softmax(attn_weights, dim=1)
        
        # Weighted sum
        context = torch.sum(attn_weights * gru_out, dim=1)  # (B, H*2)
        
        # Classify
        logits = self.classifier(context)  # (B, 2)
        
        return logits


# %%
# Initialize model
model = BiGRUVulnDetector(
    vocab_size=config.vocab_size,
    embed_dim=config.embed_dim,
    hidden_dim=config.hidden_dim,
    num_layers=config.num_layers,
    rnn_dropout=config.rnn_dropout,
    classifier_dropout=config.classifier_dropout,
    bidirectional=config.bidirectional
)

# Multi-GPU with DataParallel
if N_GPUS > 1:
    print(f"Using DataParallel on {N_GPUS} GPUs")
    model = nn.DataParallel(model)

model = model.to(DEVICE)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nModel parameters: {total_params:,} total, {trainable_params:,} trainable")

# %% [markdown]
# ## 5. Training Utilities

# %%
class LabelSmoothingCrossEntropy(nn.Module):
    """Cross entropy with label smoothing"""
    
    def __init__(self, smoothing: float = 0.1, weight: torch.Tensor = None):
        super().__init__()
        self.smoothing = smoothing
        self.weight = weight
    
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        n_classes = pred.size(-1)
        log_preds = F.log_softmax(pred, dim=-1)
        
        # Smooth labels
        with torch.no_grad():
            smooth_labels = torch.zeros_like(log_preds)
            smooth_labels.fill_(self.smoothing / (n_classes - 1))
            smooth_labels.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)
        
        # Weighted loss
        loss = -smooth_labels * log_preds
        if self.weight is not None:
            weight = self.weight[target]
            loss = loss.sum(dim=-1) * weight
            return loss.mean()
        return loss.sum(dim=-1).mean()


class EarlyStopping:
    """Early stopping to prevent overfitting"""
    
    def __init__(self, patience: int = 7, min_delta: float = 1e-4):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.should_stop = False
    
    def __call__(self, score: float) -> bool:
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
        else:
            self.best_score = score
            self.counter = 0
        
        return self.should_stop


class MetricsTracker:
    """Track and compute training metrics"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.losses = []
        self.predictions = []
        self.labels = []
        self.probabilities = []
    
    def update(
        self, 
        loss: float, 
        preds: np.ndarray, 
        labels: np.ndarray,
        probs: np.ndarray
    ):
        self.losses.append(loss)
        self.predictions.extend(preds.tolist())
        self.labels.extend(labels.tolist())
        self.probabilities.extend(probs.tolist())
    
    def compute(self) -> Dict[str, float]:
        preds = np.array(self.predictions)
        labels = np.array(self.labels)
        probs = np.array(self.probabilities)
        
        metrics = {
            'loss': np.mean(self.losses),
            'accuracy': accuracy_score(labels, preds),
            'precision': precision_score(labels, preds, zero_division=0),
            'recall': recall_score(labels, preds, zero_division=0),
            'f1': f1_score(labels, preds, zero_division=0),
        }
        
        # AUC-ROC (only if both classes present)
        if len(np.unique(labels)) > 1:
            metrics['auc_roc'] = roc_auc_score(labels, probs)
        else:
            metrics['auc_roc'] = 0.0
        
        return metrics


def save_checkpoint(
    model: nn.Module,
    optimizer: optim.Optimizer,
    epoch: int,
    metrics: Dict,
    path: str
):
    """Save model checkpoint"""
    model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
    
    torch.save({
        'epoch': epoch,
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'config': config.to_dict()
    }, path)


def load_checkpoint(path: str, model: nn.Module, optimizer: optim.Optimizer = None):
    """Load model checkpoint"""
    checkpoint = torch.load(path, map_location=DEVICE, weights_only=False)
    
    if isinstance(model, nn.DataParallel):
        model.module.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    return checkpoint['epoch'], checkpoint['metrics']

# %% [markdown]
# ## 6. Training Loop with AMP & Gradient Accumulation

# %%
def train_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: optim.Optimizer,
    criterion: nn.Module,
    scaler: GradScaler,
    grad_clip: float,
    accumulation_steps: int = 1,
    use_amp: bool = True
) -> Dict[str, float]:
    """Train one epoch with AMP and gradient accumulation"""
    model.train()
    tracker = MetricsTracker()
    
    optimizer.zero_grad()
    
    pbar = tqdm(loader, desc="Training", leave=False)
    for batch_idx, batch in enumerate(pbar):
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)
        
        # Forward with AMP
        with autocast(enabled=use_amp):
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            loss = loss / accumulation_steps  # Scale for accumulation
        
        # Backward with scaler
        scaler.scale(loss).backward()
        
        # Step optimizer every accumulation_steps
        if (batch_idx + 1) % accumulation_steps == 0:
            if grad_clip > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        # Track metrics (use unscaled loss for logging)
        with torch.no_grad():
            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
            preds = logits.argmax(dim=1).cpu().numpy()
        
        tracker.update(
            loss.item() * accumulation_steps,  # Unscale for logging
            preds,
            labels.cpu().numpy(),
            probs
        )
        
        pbar.set_postfix({'loss': f'{loss.item() * accumulation_steps:.4f}'})
    
    return tracker.compute()


@torch.no_grad()
def evaluate(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    use_amp: bool = True
) -> Dict[str, float]:
    """Evaluate model"""
    model.eval()
    tracker = MetricsTracker()
    
    for batch in tqdm(loader, desc="Evaluating", leave=False):
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)
        
        with autocast(enabled=use_amp):
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
        
        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
        preds = logits.argmax(dim=1).cpu().numpy()
        
        tracker.update(
            loss.item(),
            preds,
            labels.cpu().numpy(),
            probs
        )
    
    return tracker.compute()


# %%
def train(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    config: TrainConfig
) -> Dict:
    """Full training loop with all best practices"""
    
    # Class weights for imbalanced data
    labels = train_loader.dataset.labels
    class_counts = np.bincount(labels)
    class_weights = torch.tensor(
        len(labels) / (len(class_counts) * class_counts),
        dtype=torch.float
    ).to(DEVICE)
    print(f"Class weights: {class_weights.cpu().numpy()}")
    
    # Loss with label smoothing
    if config.label_smoothing > 0:
        criterion = LabelSmoothingCrossEntropy(
            smoothing=config.label_smoothing, 
            weight=class_weights
        )
    else:
        criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # Optimizer
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    # Scheduler
    if config.scheduler_type == 'onecycle':
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=config.max_lr,
            steps_per_epoch=len(train_loader) // config.accumulation_steps,
            epochs=config.max_epochs,
            pct_start=0.1,
            anneal_strategy='cos'
        )
    else:
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3
        )
    
    # AMP scaler
    scaler = GradScaler(enabled=config.use_amp)
    
    early_stopping = EarlyStopping(config.patience, config.min_delta)
    
    history = {'train': [], 'val': []}
    best_f1 = 0.0
    best_epoch = 0
    
    print("\n" + "="*60)
    print("Starting training...")
    print(f"  Effective batch size: {config.batch_size * config.accumulation_steps}")
    print(f"  Mixed precision: {config.use_amp}")
    print(f"  Scheduler: {config.scheduler_type}")
    print("="*60)
    
    for epoch in range(1, config.max_epochs + 1):
        epoch_start = time.time()
        
        # Train
        train_metrics = train_epoch(
            model, train_loader, optimizer, criterion, scaler,
            config.grad_clip, config.accumulation_steps, config.use_amp
        )
        
        # Validate
        val_metrics = evaluate(model, val_loader, criterion, config.use_amp)
        
        epoch_time = time.time() - epoch_start
        
        # Log
        history['train'].append(train_metrics)
        history['val'].append(val_metrics)
        
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nEpoch {epoch}/{config.max_epochs} ({epoch_time:.1f}s) - LR: {current_lr:.2e}")
        print(f"  Train - Loss: {train_metrics['loss']:.4f}, F1: {train_metrics['f1']:.4f}, AUC: {train_metrics['auc_roc']:.4f}")
        print(f"  Val   - Loss: {val_metrics['loss']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc_roc']:.4f}")
        
        # Learning rate scheduling
        if config.scheduler_type == 'plateau':
            scheduler.step(val_metrics['f1'])
        # OneCycleLR steps per batch in train_epoch via step() - but we step per epoch here for simplicity
        
        # Save best model
        if val_metrics['f1'] > best_f1:
            best_f1 = val_metrics['f1']
            best_epoch = epoch
            save_checkpoint(
                model, optimizer, epoch, val_metrics,
                os.path.join(MODEL_DIR, 'best_model.pt')
            )
            print(f"  âœ“ New best model saved (F1: {best_f1:.4f})")
        
        # Regular checkpoint
        if epoch % config.save_every == 0:
            save_checkpoint(
                model, optimizer, epoch, val_metrics,
                os.path.join(MODEL_DIR, f'checkpoint_epoch_{epoch}.pt')
            )
        
        # Early stopping
        if early_stopping(val_metrics['f1']):
            print(f"\nEarly stopping at epoch {epoch}")
            break
    
    print("\n" + "="*60)
    print(f"Training complete! Best F1: {best_f1:.4f} at epoch {best_epoch}")
    print("="*60)
    
    # Save history
    with open(os.path.join(LOG_DIR, 'training_history.json'), 'w') as f:
        json.dump(history, f, indent=2)
    
    return history

# %% [markdown]
# ## 7. Run Training

# %%
history = train(model, train_loader, val_loader, config)

# %% [markdown]
# ## 8. Final Evaluation on Test Set

# %%
# Load best model
print("Loading best model for evaluation...")
load_checkpoint(os.path.join(MODEL_DIR, 'best_model.pt'), model)

# Evaluate on test set
class_weights = None  # Use unweighted for final eval
criterion = nn.CrossEntropyLoss()
test_metrics = evaluate(model, test_loader, criterion, config.use_amp)

print("\n" + "="*60)
print("TEST SET RESULTS")
print("="*60)
print(f"Accuracy:  {test_metrics['accuracy']:.4f}")
print(f"Precision: {test_metrics['precision']:.4f}")
print(f"Recall:    {test_metrics['recall']:.4f}")
print(f"F1 Score:  {test_metrics['f1']:.4f}")
print(f"AUC-ROC:   {test_metrics['auc_roc']:.4f}")

# %%
# Detailed classification report
@torch.no_grad()
def get_predictions(model, loader, use_amp=True):
    model.eval()
    all_preds, all_labels, all_probs = [], [], []
    
    for batch in loader:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        
        with autocast(enabled=use_amp):
            logits = model(input_ids, attention_mask)
        probs = torch.softmax(logits, dim=1)[:, 1]
        preds = logits.argmax(dim=1)
        
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch['labels'].numpy())
        all_probs.extend(probs.cpu().numpy())
    
    return np.array(all_preds), np.array(all_labels), np.array(all_probs)

preds, labels, probs = get_predictions(model, test_loader, config.use_amp)

print("\nClassification Report:")
print(classification_report(labels, preds, target_names=['Non-Vulnerable', 'Vulnerable']))

print("\nConfusion Matrix:")
cm = confusion_matrix(labels, preds)
print(f"  TN: {cm[0,0]:5d}  FP: {cm[0,1]:5d}")
print(f"  FN: {cm[1,0]:5d}  TP: {cm[1,1]:5d}")

# %% [markdown]
# ## 9. Training Curves

# %%
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

epochs = range(1, len(history['train']) + 1)

# Loss
axes[0].plot(epochs, [m['loss'] for m in history['train']], 'b-', label='Train')
axes[0].plot(epochs, [m['loss'] for m in history['val']], 'r-', label='Val')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss')
axes[0].legend()
axes[0].grid(True)

# F1
axes[1].plot(epochs, [m['f1'] for m in history['train']], 'b-', label='Train')
axes[1].plot(epochs, [m['f1'] for m in history['val']], 'r-', label='Val')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('F1 Score')
axes[1].set_title('F1 Score')
axes[1].legend()
axes[1].grid(True)

# AUC-ROC
axes[2].plot(epochs, [m['auc_roc'] for m in history['train']], 'b-', label='Train')
axes[2].plot(epochs, [m['auc_roc'] for m in history['val']], 'r-', label='Val')
axes[2].set_xlabel('Epoch')
axes[2].set_ylabel('AUC-ROC')
axes[2].set_title('AUC-ROC')
axes[2].legend()
axes[2].grid(True)

plt.tight_layout()
plt.savefig(os.path.join(LOG_DIR, 'training_curves.png'), dpi=150)
plt.show()

# %% [markdown]
# ## 10. Export Model for Inference

# %%
# Save final model with config
final_export = {
    'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
    'config': config.to_dict(),
    'test_metrics': test_metrics,
    'timestamp': datetime.now().isoformat(),
    'model_type': 'BiGRU'
}
torch.save(final_export, os.path.join(MODEL_DIR, 'bigru_vuln_detector_final.pt'))
print(f"\nFinal model exported to {MODEL_DIR}/bigru_vuln_detector_final.pt")

# %% [markdown]
# ## Summary
# 
# **BiGRU Model Improvements over LSTM baseline:**
# 
# 1. **Architecture**:
#    - BiGRU (25% fewer params than BiLSTM)
#    - Embedding dropout (0.1)
#    - Strong classifier dropout (0.5)
#    - GELU activation
# 
# 2. **Training**:
#    - Mixed precision (AMP) for faster training
#    - Gradient accumulation (effective batch 128)
#    - OneCycleLR for better convergence
#    - Label smoothing (0.05) for noisy labels
#    - Weight decay (5e-3) for regularization
# 
# 3. **Hyperparameters** (optimized for vocab=266, 21K samples):
#    - embed_dim: 128 (was 256)
#    - hidden_dim: 256 (BiGRU output = 512)
#    - weight_decay: 5e-3 (was 1e-5)
#    - dropout: 0.3 RNN, 0.5 classifier (was 0.3)
# 
# **Next Steps:**
# 1. Try CodeBERT/GraphCodeBERT for better performance
# 2. Add graph-based features (CFG/DFG)
# 3. Ensemble sequence + graph models
