# Ph√¢n T√≠ch v√† C·∫£i Ti·∫øn H·ªá Th·ªëng Ph√°t Hi·ªán L·ªó H·ªïng C Code
## BiGRU Vulnerability Detection - Devign Dataset

**Ng√†y ph√¢n t√≠ch:** 22/12/2024  
**Hi·ªáu su·∫•t hi·ªán t·∫°i:** F1 ~72%, AUC-ROC ~82%  
**M·ª•c ti√™u:** F1 ‚â• 75%, AUC-ROC ‚â• 85%

---

## üìä I. NH·∫¨N X√âT (Observations)

### 1. Ki·∫øn Tr√∫c Hybrid BiGRU + V2 Features

#### ‚úÖ ƒêi·ªÉm m·∫°nh
| Component | M√¥ t·∫£ | ƒê√°nh gi√° |
|-----------|-------|----------|
| **Dual-branch architecture** | BiGRU cho tokens + MLP cho V2 features | Thi·∫øt k·∫ø h·ª£p l√Ω, k·∫øt h·ª£p ng·ªØ nghƒ©a v√† tri th·ª©c tƒ©nh |
| **Multi-head attention pooling** | 4-6 heads thay v√¨ simple pooling | Gi√∫p t·∫≠p trung v√†o ƒëo·∫°n code quan tr·ªçng |
| **V2 Features "Missing Defenses"** | ƒê·∫øm thi·∫øu ph√≤ng th·ªß thay v√¨ ch·ªâ ƒë·∫øm nguy hi·ªÉm | Ph√π h·ª£p v·ªõi c√°ch audit code th·ª±c t·∫ø |
| **SWA + Ensemble** | 5-7 models v·ªõi dropout variations | TƒÉng robustness v√† generalization |

#### ‚ö†Ô∏è ƒêi·ªÉm y·∫øu
| V·∫•n ƒë·ªÅ | Chi ti·∫øt | T√°c ƒë·ªông |
|--------|----------|----------|
| **Kh√¥ng t·∫≠n d·ª•ng ƒë·ªì th·ªã** | AST/CFG/DFG ch·ªâ ƒë∆∞·ª£c n√©n th√†nh vector th·ªëng k√™ | B·ªè ph√≠ compute cho graph m√† kh√¥ng d√πng GNN/path encoding |
| **Vocab qu√° compact** | ~266 tokens v·ªõi normalize_vars=True | H·∫°n ch·∫ø kh·∫£ nƒÉng ph√¢n bi·ªát patterns tinh vi |
| **V2 features global** | Kh√¥ng g·∫Øn v·ªõi v·ªã tr√≠ trong sequence | BiGRU kh√¥ng bi·∫øt token n√†o li√™n quan ƒë·∫øn "missing defense" |
| **Regex-based detection** | Nhi·ªÅu logic V2 d·ª±a v√†o regex ƒë∆°n gi·∫£n | False positive/negative cao, noise gi·ªõi h·∫°n tr·∫ßn m√¥ h√¨nh |

### 2. Pipeline Preprocessing (10 b∆∞·ªõc)

```
load ‚Üí vuln_features ‚Üí ast ‚Üí cfg ‚Üí dfg ‚Üí slice ‚Üí tokenize ‚Üí normalize ‚Üí vocab ‚Üí vectorize
```

#### ‚úÖ ƒêi·ªÉm m·∫°nh
- **Checkpointing & Chunking**: Resume ƒë∆∞·ª£c khi b·ªã interrupt
- **GC sau m·ªói chunk**: Qu·∫£n l√Ω b·ªô nh·ªõ t·ªët tr√™n Kaggle
- **joblib parallelization**: TƒÉng t·ªëc x·ª≠ l√Ω
- **ƒê·∫ßy ƒë·ªß graph stats**: `cfg_block_count`, `dfg_node_count`, etc.

#### ‚ö†Ô∏è ƒêi·ªÉm y·∫øu
| V·∫•n ƒë·ªÅ | Gi·∫£i th√≠ch |
|--------|------------|
| **Tokenization tuy·∫øn t√≠nh** | AST/CFG/DFG kh√¥ng ƒë∆∞·ª£c d√πng ƒë·ªÉ x√¢y sequence structure cao h∆°n |
| **Slicing c·ª©ng nh·∫Øc** | M·ªôt slice duy nh·∫•t, b·ªè qua multi-view (forward + backward) |
| **window_size c·ªë ƒë·ªãnh** | Kh√¥ng adaptive theo ƒë·ªô d√†i h√†m |
| **Slicing kh√¥ng bi·∫øt V2** | Kh√¥ng t·∫≠n d·ª•ng V2 features ƒë·ªÉ ch·ªçn ƒëo·∫°n code quan tr·ªçng |

### 3. Code Slicing Strategy

#### Hi·ªán t·∫°i
- **Backward slice**: Default, d·ª±a tr√™n `vul_lines` 
- **Forward slice**: C√≥ s·∫µn nh∆∞ng √≠t d√πng
- **Window fallback**: ¬±15 d√≤ng khi parse fail

#### V·∫•n ƒë·ªÅ
```
H√†m d√†i (LOC > 200) ‚Üí Slice qu√° to ‚Üí Nhi·ªÅu noise
H√†m ng·∫Øn (LOC < 50) ‚Üí Window g·∫ßn nh∆∞ full h√†m ‚Üí Kh√¥ng c√≥ √≠ch
Kh√¥ng c√≥ vul_lines ‚Üí Fallback full code ‚Üí Attention kh√≥ h·ªçc
```

### 4. V2 Features - Missing Defenses

#### Danh s√°ch features hi·ªán c√≥
```python
# Dangerous calls
dangerous_call_without_check_count/ratio

# Pointer operations  
pointer_deref_without_null_check_count/ratio

# Array access
array_access_without_bounds_check_count/ratio

# Memory management
malloc_without_free_count/ratio
free_without_null_check_count/ratio

# Return values
unchecked_return_value_count/ratio

# Defense metrics
defense_ratio
null_check_density
```

#### Ph√¢n t√≠ch
- **∆Øu ƒëi·ªÉm**: Capture ƒë∆∞·ª£c pattern quan tr·ªçng, g·∫ßn v·ªõi c√°ch human audit
- **Nh∆∞·ª£c ƒëi·ªÉm**: 
  - Kh√¥ng path-sensitive, kh√¥ng fully control/data-flow-sensitive
  - T·∫•t c·∫£ features ƒë·ªÅu global tr√™n to√†n h√†m/slice
  - Kh√¥ng c√≥ feature selection hay scaling chuy√™n bi·ªát

### 5. Training Configuration Analysis

#### Progression c·ªßa c√°c configs
```
TrainConfig ‚Üí LargeTrainConfig ‚Üí RegularizedConfig ‚Üí ImprovedConfig 
    ‚Üí EnhancedConfig ‚Üí OptimizedConfig ‚Üí RefinedConfig ‚Üí FinalConfig 
    ‚Üí QuickWinConfig ‚Üí AdvancedConfig ‚Üí AdvancedConfigV2 ‚Üí AdvancedConfigV3
```

#### K·∫øt qu·∫£ th·ª±c nghi·ªám
| Config | F1 | AUC | Precision | Recall | Ghi ch√∫ |
|--------|-----|-----|-----------|--------|---------|
| Baseline | ~72% | ~82% | ~49% | ~90% | High recall, low precision |
| AdvancedV2 | ~66% | ~80% | ~75-81% | ~50-59% | Recall collapsed |
| AdvancedV3 | ~72% | ~82% | - | - | Restored but plateaued |

#### K·∫øt lu·∫≠n
> **Bottleneck kh√¥ng c√≤n ·ªü optimizer/hyperparam** m√† ·ªü **representation** (model & features, c√°ch encode AST/CFG/DFG/slices)

---

## üöÄ II. C·∫¢I TI·∫æN (Improvements)

### M·ª©c 1: Thay ƒë·ªïi √≠t - T√°c ƒë·ªông nhanh ‚ö°

#### 1.1 Multi-Slice / Multi-Instance Learning

**√ù t∆∞·ªüng**: Thay v√¨ 1 slice/h√†m, t·∫°o nhi·ªÅu slices v·ªõi perspectives kh√°c nhau

```python
# Pseudo-code
class MultiSliceDataset(Dataset):
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # T·∫°o nhi·ªÅu slices
        slices = []
        for vul_line in sample['vul_lines']:
            slices.append(backward_slice(sample['code'], vul_line))
            slices.append(forward_slice(sample['code'], vul_line))
        
        # Fallback window n·∫øu kh√¥ng c√≥ vul_lines
        if not slices:
            slices.append(window_slice(sample['code']))
        
        return {
            'slice_tokens': [tokenize(s) for s in slices],
            'slice_count': len(slices),
            'v2_features': sample['v2_features'],
            'label': sample['label']
        }

# Model v·ªõi slice-level attention
class MultiSliceModel(nn.Module):
    def forward(self, batch):
        # Encode m·ªói slice
        slice_embeddings = []
        for slice_tokens in batch['slice_tokens']:
            h = self.bigru_encoder(slice_tokens)
            slice_embed = self.token_attention_pool(h)
            slice_embeddings.append(slice_embed)
        
        # Attention over slices (multi-instance)
        stacked = torch.stack(slice_embeddings)
        final_embed = self.slice_attention_pool(stacked)
        
        return self.classifier(final_embed)
```

**L·ª£i √≠ch**:
- T√°ch ri√™ng contexts: backward capture nguy√™n nh√¢n, forward capture h·∫≠u qu·∫£
- Model c√≥ th·ªÉ h·ªçc b·ªè qua slice noisy th√¥ng qua attention

#### 1.2 Distance-to-Criterion Token Feature

**√ù t∆∞·ªüng**: Th√™m positional feature cho bi·∫øt token g·∫ßn vul_line bao nhi√™u

```python
def compute_distance_feature(tokens, token_lines, criterion_lines):
    """
    T√≠nh kho·∫£ng c√°ch t·ª´ m·ªói token ƒë·∫øn vul_line g·∫ßn nh·∫•t
    """
    distances = []
    for line in token_lines:
        min_dist = min(abs(line - c) for c in criterion_lines)
        distances.append(min_dist)
    
    # Normalize v√† embed
    max_dist = 20  # clamp
    normalized = [min(d / max_dist, 1.0) for d in distances]
    return normalized

# Trong model
class EnhancedBiGRU(nn.Module):
    def __init__(self, ...):
        self.dist_embedding = nn.Linear(1, 16)  # Ho·∫∑c embedding table
        
    def forward(self, tokens, distance_features):
        token_embed = self.token_embedding(tokens)
        dist_embed = self.dist_embedding(distance_features.unsqueeze(-1))
        
        # Concatenate
        combined = torch.cat([token_embed, dist_embed], dim=-1)
        return self.bigru(combined)
```

**L·ª£i √≠ch**: Gi√∫p attention ∆∞u ti√™n tokens g·∫ßn v√πng nghi v·∫•n

#### 1.3 Adaptive Slicing Parameters

```python
def get_adaptive_slice_config(code):
    loc = len(code.split('\n'))
    
    if loc > 200:
        return SliceConfig(window_size=10, max_depth=3)
    elif loc > 100:
        return SliceConfig(window_size=15, max_depth=4)
    elif loc < 50:
        return SliceConfig(window_size=loc, max_depth=5)  # Full function
    else:
        return SliceConfig(window_size=15, max_depth=5)  # Default
```

---

### M·ª©c 2: C·∫£i thi·ªán V2 Features üìä

#### 2.1 Feature Scaling & Selection

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

# C√°c ratio features th∆∞·ªùng r·∫•t skewed (nhi·ªÅu 0, m·ªôt √≠t 1)
RATIO_FEATURES = [
    'malloc_without_free_ratio',
    'free_without_null_check_ratio', 
    'array_access_without_bounds_check_ratio',
    'dangerous_call_without_check_ratio',
    'pointer_deref_without_null_check_ratio',
]

def transform_v2_features(features):
    transformed = {}
    for key, value in features.items():
        if key in RATIO_FEATURES:
            # Log transform for skewed distributions
            transformed[key] = np.log1p(value * 10)  # Scale up before log
        elif '_count' in key:
            # Log transform for counts
            transformed[key] = np.log1p(value)
        else:
            transformed[key] = value
    return transformed

# Feature importance analysis
from sklearn.ensemble import RandomForestClassifier

def analyze_feature_importance(X_v2, y):
    """Ch·∫°y RF ƒë·ªÉ ƒëo feature importance"""
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_v2, y)
    
    importance = dict(zip(X_v2.columns, rf.feature_importances_))
    return sorted(importance.items(), key=lambda x: -x[1])
```

#### 2.2 Localized V2 Features (per-slice)

```python
def extract_v2_features_per_slice(slice_code, original_code, dictionary):
    """
    Compute V2 features tr√™n slice thay v√¨ full function
    ‚Üí Gi·∫£m noise, tƒÉng alignment v·ªõi context BiGRU nh√¨n
    """
    # Local features on slice
    slice_features = extract_vuln_features_v2(slice_code, dictionary)
    
    # Global features for context
    global_features = extract_vuln_features_v2(original_code, dictionary)
    
    # Combined: slice features + relative metrics
    combined = {
        # Slice-level
        **{f'slice_{k}': v for k, v in slice_features.items()},
        
        # Slice-to-global ratios
        'slice_loc_ratio': slice_features['loc'] / max(global_features['loc'], 1),
        'slice_danger_concentration': (
            slice_features['dangerous_call_count'] / 
            max(global_features['dangerous_call_count'], 1)
        ),
    }
    return combined
```

#### 2.3 Additional Graph-Level Features

```python
def compute_graph_complexity_features(cfg_stats, dfg_stats):
    """
    Th√™m complexity metrics t·ª´ CFG/DFG stats c√≥ s·∫µn
    """
    cfg_blocks = cfg_stats.get('block_count', 0)
    cfg_edges = cfg_stats.get('edge_count', 0)
    dfg_nodes = dfg_stats.get('node_count', 0)
    dfg_edges = dfg_stats.get('edge_count', 0)
    dfg_defs = dfg_stats.get('def_count', 0)
    dfg_uses = dfg_stats.get('use_count', 0)
    
    return {
        # Cyclomatic complexity proxy
        'cyclomatic_complexity': cfg_edges - cfg_blocks + 2,
        
        # DFG density
        'dfg_avg_degree': (2 * dfg_edges) / max(dfg_nodes, 1),
        
        # Def-use ratio (high ratio = complex data flow)
        'dfg_def_use_ratio': dfg_defs / max(dfg_uses, 1),
        
        # CFG complexity
        'cfg_branching_factor': cfg_edges / max(cfg_blocks, 1),
    }
```

---

### M·ª©c 3: N√¢ng c·∫•p Model Architecture üèóÔ∏è

#### 3.1 Hierarchical Encoding (Statement-Level)

```python
class HierarchicalBiGRU(nn.Module):
    """
    Level 1: Encode m·ªói statement ri√™ng
    Level 2: BiGRU over statement embeddings
    """
    def __init__(self, vocab_size, embed_dim=64, stmt_hidden=128, doc_hidden=192):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # Statement-level encoder (CNN ho·∫∑c BiGRU nh·ªè)
        self.stmt_encoder = nn.LSTM(embed_dim, stmt_hidden//2, 
                                     bidirectional=True, batch_first=True)
        
        # Document-level encoder  
        self.doc_encoder = nn.GRU(stmt_hidden, doc_hidden//2,
                                   bidirectional=True, batch_first=True)
        
        # Attention pooling
        self.stmt_attention = nn.MultiheadAttention(stmt_hidden, 4)
        self.doc_attention = nn.MultiheadAttention(doc_hidden, 4)
        
    def forward(self, statements_batch):
        """
        statements_batch: [batch, max_stmts, max_tokens]
        """
        B, S, T = statements_batch.shape
        
        # Encode each statement
        stmt_embeds = []
        for s in range(S):
            tokens = statements_batch[:, s, :]  # [B, T]
            x = self.embedding(tokens)  # [B, T, E]
            h, _ = self.stmt_encoder(x)  # [B, T, H]
            # Attention pool over tokens
            pooled = self._attention_pool(h, self.stmt_attention)  # [B, H]
            stmt_embeds.append(pooled)
        
        # Stack statements: [B, S, H]
        stmt_seq = torch.stack(stmt_embeds, dim=1)
        
        # Encode statement sequence
        doc_h, _ = self.doc_encoder(stmt_seq)  # [B, S, D]
        
        # Final attention pool
        output = self._attention_pool(doc_h, self.doc_attention)  # [B, D]
        
        return output
```

**L·ª£i √≠ch**:
- Match c·∫•u tr√∫c logic c·ªßa AST/CFG t·ªët h∆°n
- Gi·∫£m sequence length (512 tokens ‚Üí ~50 statements)
- D·ªÖ h·ªçc dependencies d√†i h∆°n

#### 3.2 Light-weight Transformer Encoder

```python
class LightTransformerEncoder(nn.Module):
    """
    2-3 layer Transformer, c√≥ th·ªÉ d√πng song song v·ªõi BiGRU
    """
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.pos_encoding = PositionalEncoding(d_model, max_len=512)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            dim_feedforward=d_model*4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Attention pooling
        self.pool = nn.Linear(d_model, 1)
        
    def forward(self, tokens, attention_mask):
        x = self.embedding(tokens)
        x = self.pos_encoding(x)
        
        # Create transformer mask
        mask = (attention_mask == 0)  # True where padded
        
        h = self.transformer(x, src_key_padding_mask=mask)
        
        # Attention pooling
        weights = self.pool(h).squeeze(-1)  # [B, T]
        weights = weights.masked_fill(mask, float('-inf'))
        weights = F.softmax(weights, dim=-1)
        
        output = (h * weights.unsqueeze(-1)).sum(dim=1)
        return output
```

#### 3.3 GNN Branch tr√™n DFG (Branch th·ª© 3)

```python
import torch_geometric
from torch_geometric.nn import GCNConv, GATConv, global_attention_pool

class DFGGraphBranch(nn.Module):
    """
    Nh·ªè g·ªçn: 2 layer GCN/GAT, hidden 64-96
    """
    def __init__(self, node_features=32, hidden=64, output_dim=128):
        super().__init__()
        self.node_embed = nn.Linear(node_features, hidden)
        
        self.conv1 = GATConv(hidden, hidden, heads=2, concat=False, dropout=0.3)
        self.conv2 = GATConv(hidden, hidden, heads=2, concat=False, dropout=0.3)
        
        # Global attention pooling
        self.gate_nn = nn.Linear(hidden, 1)
        self.output = nn.Linear(hidden, output_dim)
        
    def forward(self, x, edge_index, batch):
        """
        x: node features [N, F]
        edge_index: [2, E]
        batch: batch assignment [N]
        """
        x = F.relu(self.node_embed(x))
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.3, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        
        # Global attention pooling
        pooled = global_attention_pool(x, batch, self.gate_nn)
        
        return self.output(pooled)

# Combined model
class HybridModelWithGNN(nn.Module):
    def __init__(self, ...):
        self.token_branch = BiGRUEncoder(...)
        self.v2_branch = MLPBranch(...)
        self.graph_branch = DFGGraphBranch(...)
        
        # Fusion
        total_dim = token_dim + v2_dim + graph_dim
        self.classifier = nn.Sequential(
            nn.Linear(total_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2)
        )
        
    def forward(self, tokens, v2_features, dfg_data):
        token_embed = self.token_branch(tokens)
        v2_embed = self.v2_branch(v2_features)
        graph_embed = self.graph_branch(
            dfg_data.x, dfg_data.edge_index, dfg_data.batch
        )
        
        combined = torch.cat([token_embed, v2_embed, graph_embed], dim=-1)
        return self.classifier(combined)
```

---

### M·ª©c 4: Training Strategy Improvements üìà

#### 4.1 K-Fold Cross-Validation Ensemble

```python
from sklearn.model_selection import StratifiedKFold

def train_kfold_ensemble(data, labels, n_folds=5):
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    fold_models = []
    oof_predictions = np.zeros(len(labels))
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(data, labels)):
        print(f"Training Fold {fold+1}/{n_folds}")
        
        train_data = data[train_idx]
        val_data = data[val_idx]
        
        model = build_model(config)
        train_fold(model, train_data, val_data, ...)
        
        # Out-of-fold predictions
        oof_predictions[val_idx] = model.predict_proba(val_data)[:, 1]
        
        fold_models.append(model)
    
    # Ensemble: average predictions
    return fold_models, oof_predictions
```

**L·ª£i √≠ch**: Gi·∫£m variance do split, th∆∞·ªùng tƒÉng AUC/F1 v√†i ƒëi·ªÉm

#### 4.2 Probability Calibration

```python
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression

def calibrate_model(model, val_loader):
    """Temperature scaling ho·∫∑c Isotonic regression"""
    
    # Collect logits and labels
    all_logits = []
    all_labels = []
    
    model.eval()
    with torch.no_grad():
        for batch in val_loader:
            logits = model(batch)
            all_logits.append(logits[:, 1].cpu().numpy())  # Positive class logit
            all_labels.append(batch['labels'].cpu().numpy())
    
    logits = np.concatenate(all_logits)
    labels = np.concatenate(all_labels)
    
    # Fit isotonic regression
    ir = IsotonicRegression(out_of_bounds='clip')
    probs = 1 / (1 + np.exp(-logits))  # Sigmoid
    ir.fit(probs, labels)
    
    return ir

def predict_calibrated(model, data, calibrator):
    logits = model(data)
    probs = torch.sigmoid(logits[:, 1]).cpu().numpy()
    calibrated_probs = calibrator.transform(probs)
    return calibrated_probs
```

**L·ª£i √≠ch**: AUC t·ªët nh∆∞ng F1 ch∆∞a t·ªët th∆∞·ªùng do calibration k√©m ‚Üí c·∫£i thi·ªán 1-2 ƒëi·ªÉm F1

#### 4.3 Curriculum Learning

```python
def compute_sample_difficulty(v2_features):
    """
    Difficulty score d·ª±a tr√™n V2 features
    Samples v·ªõi ratio cao = "d·ªÖ" (r√µ r√†ng vulnerable)
    Samples v·ªõi ratio th·∫•p = "kh√≥" (subtle)
    """
    danger_score = (
        v2_features['dangerous_call_without_check_ratio'] +
        v2_features['pointer_deref_without_null_check_ratio'] +
        v2_features['array_access_without_bounds_check_ratio']
    ) / 3
    
    # Invert: high danger = easy, low danger = hard
    difficulty = 1 - danger_score
    return difficulty

def curriculum_sampler(dataset, epoch, max_epochs):
    """
    Giai ƒëo·∫°n ƒë·∫ßu: focus easy samples
    Giai ƒëo·∫°n sau: th√™m d·∫ßn hard samples
    """
    difficulties = [compute_sample_difficulty(s['v2_features']) for s in dataset]
    
    # Progress ratio
    progress = epoch / max_epochs
    
    # Sampling weights: easy samples get higher weight early
    weights = []
    for d in difficulties:
        if progress < 0.3:
            # Early: prefer easy (difficulty < 0.5)
            w = 1.0 if d < 0.5 else 0.3
        elif progress < 0.6:
            # Mid: balanced
            w = 1.0
        else:
            # Late: slight preference for hard samples
            w = 1.5 if d > 0.5 else 1.0
        weights.append(w)
    
    return WeightedRandomSampler(weights, len(weights))
```

---

### M·ª©c 5: Advanced Techniques üî¨

#### 5.1 Self-Supervised Pretraining

```python
class MaskedLanguageModel(nn.Module):
    """
    Pretrain BiGRU/Transformer v·ªõi Masked LM tr√™n unlabeled code
    """
    def __init__(self, vocab_size, d_model=256):
        super().__init__()
        self.encoder = BiGRUEncoder(vocab_size, d_model)
        self.mlm_head = nn.Linear(d_model * 2, vocab_size)  # BiGRU hidden*2
        
    def forward(self, tokens, mask_positions):
        h = self.encoder(tokens)  # [B, T, H]
        masked_h = h[mask_positions]  # [N_masked, H]
        logits = self.mlm_head(masked_h)  # [N_masked, vocab_size]
        return logits

def pretrain_mlm(model, unlabeled_data, epochs=10):
    """
    Mask 15% tokens, predict original
    """
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(epochs):
        for batch in unlabeled_data:
            tokens = batch['tokens']
            
            # Create mask (15% of non-padding tokens)
            mask = create_random_mask(tokens, mask_ratio=0.15)
            masked_tokens = tokens.clone()
            masked_tokens[mask] = MASK_TOKEN_ID
            
            # Forward
            logits = model(masked_tokens, mask)
            loss = F.cross_entropy(logits, tokens[mask])
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

#### 5.2 Knowledge Distillation t·ª´ CodeBERT

```python
def distill_from_codebert(student_model, codebert, train_loader):
    """
    Train student (BiGRU) ƒë·ªÉ mimic CodeBERT embeddings
    """
    # Freeze CodeBERT
    codebert.eval()
    for p in codebert.parameters():
        p.requires_grad = False
    
    # Distillation loss
    def distill_loss(student_embed, teacher_embed, temperature=2.0):
        # Cosine similarity loss
        cos_loss = 1 - F.cosine_similarity(student_embed, teacher_embed).mean()
        
        # MSE loss
        mse_loss = F.mse_loss(student_embed, teacher_embed)
        
        return cos_loss + mse_loss
    
    optimizer = optim.Adam(student_model.parameters(), lr=5e-4)
    
    for batch in train_loader:
        # Teacher embedding
        with torch.no_grad():
            teacher_embed = codebert.encode(batch['raw_code'])
        
        # Student embedding
        student_embed = student_model.get_embedding(batch['tokens'])
        
        # Distillation + task loss
        d_loss = distill_loss(student_embed, teacher_embed)
        task_loss = F.cross_entropy(student_model(batch['tokens']), batch['labels'])
        
        total_loss = 0.5 * d_loss + 0.5 * task_loss
        
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
```

---

## üìã III. ROADMAP ƒê·ªÄ XU·∫§T

### Phase 1: Quick Wins (1-2 tu·∫ßn) ‚ö°

| Task | Expected Gain | Effort |
|------|---------------|--------|
| 1.1 Multi-slice + slice attention | +1-2% F1 | Medium |
| 1.2 Token distance-to-criterion | +0.5-1% F1 | Low |
| 2.1 Feature scaling (log-transform) | +0.5% F1 | Low |
| 4.2 Probability calibration | +1% F1 | Low |

### Phase 2: Structural Improvements (2-3 tu·∫ßn) üèóÔ∏è

| Task | Expected Gain | Effort |
|------|---------------|--------|
| 2.2 Localized V2 features | +1-2% F1/AUC | Medium |
| 4.1 K-fold cross-validation | +1% F1/AUC | Medium |
| 3.1 Hierarchical encoding | +1-2% F1 | Medium-High |

### Phase 3: Advanced Upgrades (4+ tu·∫ßn) üöÄ

| Task | Expected Gain | Effort |
|------|---------------|--------|
| 3.2 Light Transformer encoder | +1-2% F1/AUC | High |
| 3.3 GNN branch tr√™n DFG | +1-2% AUC | High |
| 5.1 Self-supervised pretraining | +2-3% F1/AUC | High |

---

## üìä IV. T·ªîNG K·∫æT

### Nguy√™n nh√¢n ch√≠nh c·ªßa plateau F1 ~72% / AUC ~82%

1. **Representation ch∆∞a ƒë·ªß m·∫°nh**: BiGRU + MLP kh√¥ng t·∫≠n d·ª•ng h·∫øt c·∫•u tr√∫c ƒë·ªì th·ªã
2. **Single-view slicing**: Ch·ªâ m·ªôt slice, b·ªè qua multi-perspective
3. **Global V2 features**: Kh√¥ng li√™n k·∫øt v·ªõi v·ªã tr√≠ trong sequence
4. **Vocabulary qu√° compact**: 266 tokens h·∫°n ch·∫ø ph√¢n bi·ªát subtle patterns

### Khuy·∫øn ngh·ªã ∆∞u ti√™n cao nh·∫•t

```
1. Multi-slice + slice-level attention (1.1) 
   + Token distance-to-criterion (1.2)
   ‚Üí ƒê√¢y l√† upgrade c√≥ t√°c ƒë·ªông l·ªõn nh·∫•t trong setting n√†y

2. Feature scaling + V2 tr√™n slice thay v√¨ full function (2.1 + 2.2)
   ‚Üí T·∫≠n d·ª•ng tri th·ª©c V2 t·ªët h∆°n, gi·∫£m noise

3. Cross-validation + calibration (4.1 + 4.2)
   ‚Üí C·∫£i thi·ªán ƒë√°ng k·ªÉ F1/AUC m√† kh√¥ng thay ki·∫øn tr√∫c
```

### Target Achievement Forecast

| Metric | Current | After Phase 1 | After Phase 2 | After Phase 3 |
|--------|---------|---------------|---------------|---------------|
| F1 | 72% | 74-75% | 76-77% | 78-80% |
| AUC-ROC | 82% | 83-84% | 85-86% | 87-89% |

---

*Document generated by Oracle analysis - 22/12/2024*
